{
  "examples": [
    {
      "task_type": "data_pipeline_setup",
      "instruction": "My project leader asks me to create an efficient data pipeline for e-commerce analytics. Our goal is to use Airbyte for data integration to transfer raw data from Faker to Snowflake, dbt for data transformation to transform raw data in Snowflake, and Snowflake for data warehousing to store final transformed data. Most codes for dbt have been finished and detailed requirements are provided in README.md of the opened project. Help me complete the remaining work.",
      "verbose_instruction": "In this task, we need to finish the data integration and transformation pipeline from Faker to Snowflake using airbyte and dbt. Firstly, we need to create two databases in the Snowflake to store the raw data and transformed data.\n1) Go to the Snowflake Snowsight UI;\n2) Click the \"Data\" menu on the left panel;\n3) On the right panel, click icon \"+ Database\" on the top right of the screen;\n4) Type in the database name \"raw_data\";\n5) Click the button \"Create\". This database is used to accept the original data from Faker source;\n6) Similarly, click the icon \"+ Database\" again to create another database called \"transformed_data\". This database is used to accept the transformed data from dbt.\nNext, we need to configure the connection in Airbyte:\n7) Switch to the Airbyte UI page in the browser;\n8) Click the button \"Create your first connection\" in the main panel;\n9) Type in \"faker\" to search the data source \"Sample Data (Faker)\";\n10) Click this source in the filtered results;\n11) Click button \"Set up source\";\n12) With respect to the Snowflake destination, type in \"Snowflake\" in the search bar;\n13) Click this destination to open the settings window;\n14) Change the default \"Authorization Method\" into \"Username and Passowrd\";\n15) According to the README.md opened in the VS Code editor, we know that some credential information is stored in the environment variables. \n16) Switch to the terminal and type in \"echo $SNOWFLAKE_HOST\". We can see the host address of Snowflake account;\n17) Get back to the web browser and type in this address in the `Host` field;\n18) Similarly, we can retrieve values in $SNOWFLAKE_USER / $SNOWFLAKE_PASSWORD, and fill in the `Username` / `Password` fields respectively;\n19) For the remaining fields, directly fill in values provided in README.md, that is:\nDatabase: raw_data\nWarehouse: COMPUTE_WH\nRole: ACCOUNTADMIN\nDefault Schema: PUBLIC\n20) After that, click the button \"Set up destination\" in the Airbyte UI page;\n21) Regarding the connection configuration, we change the \"Schedule type\" to \"Manual\";\n22) Click the button \"Set up connection\" at the bottom of the page;\n23) In the `status` panel, click the button \"Sync now\" to manually trigger the data transfer;\nNow, we can switch to the VS Code to set up the connection for dbt.\n24) Click the `dbt_project` folder in the left panel of VS Code to unfold the directory tree;\n25) Choose the `profiles.yml` file under `dbt_project` and complete the following configuration:\n```\ndbt_project:\n  outputs:\n    dev:\n      type: snowflake\n      # detailed configuration to Snowflake\n      account: '{{ env_var(\"SNOWFLAKE_ACCOUNT\") }}'\n      user: '{{ env_var(\"SNOWFLAKE_USER\") }}'\n      password: '{{ env_var(\"SNOWFLAKE_PASSWORD\") }}'\n      role: ACCOUNTADMIN\n      database: transformed_data\n      warehouse: COMPUTE_WH\n      schema: PUBLIC\n  target: dev\n```\n26) Press the hot key \"Ctrl+S\" to save the result. This profile specify the data destination of dbt transformation;\n27) With respect to the data source, we open the file with path `dbt_project -> models -> sources -> faker_sources.yml` in the VS Code;\n28) In the opened .yml file, fill in the two fields with `database: raw_data` and `schema: PUBLIC` respectively;\n29) Now, we have finished all connection work. We can change to the terminal and enter the directory `dbt_project`;\n`cd dbt_project`\n30) Type in commands `dbt debug` to check whether the connection is ok. You should see the output prompt `All checks passed!`;\n31) Then, type in commands `dbt run` in the terminal. This will perform the data transformation to convert data in database \"raw_data\" into database \"transformed_data\" on Snowflake. The output should contain the success signal `Completed successfully`.",
      "action_number": 31
    },
    {
      "task_type": "data_comparison",
      "instruction": "I have created a connection from Postgres to Snowflake. After the current sync finishes, can you help me add 2 entries to the source database with values in the opened .txt file? Then, compare the difference of replicated data with `data-diff [Snowflake] [Postgres]` and save the results into `diff_test.csv` on Desktop? The passwords for Postgres and Snowflake are stored in environment variables POSTGRES_PASSWORD and SNOWFLAKE_PASSWORD, respectively.",
      "verbose_instruction": "In this task, we want to add 2 rows with specific values to the source database and use `data-diff [Snowflake] [Postgres]` in terminal to check the difference of replicated data in the connection from Postgres to Snowflake: \n1. Switch to the browser window;\n2. Wait for the syn to finish. Then click this connection row.\n3. We need to figure out the source database name. In the connection configuration page, click the \"Postgres\" icon (with the Postgres elephant on the left) to enter the source settings.\n4. In the source settings, we can find that the source Postgres database is running locally with the following information:\nHost: localhost\nPort: 5432\nDatabase Name: development\nUsername: postgres\nWe will use these information later.\n5. Click the \"Destinations\" button on the left column panel.\n6. We can see the entry of Snowflake. Click this row.\n7. In the destination settings panel, we can see the detailed configuration like this:\nHost: https://${account}.snowflakecomputing.com\nRole: ACCOUNTADMIN\nWarehouse: COMPUTE_WH\nDatabase: DEVELOPMENT\nDefault Schema: CUSTOMERS\nUsername: ...\nFrom the \"Host\" field, we can easily extract the Snowflake ${account}. Remember these information, we will use them later.\n8. Switch to the gnome-terminal application.\n9. Type in the following codes: `cat new_values.txt`. We will see the concrete two values to add into the table \"users\".\n10. Type in `docker ps | grep \"postgres\"`. We will see the container id for this local postgresql database in the first column of terminal output.\n11. Then, type in the following shell commands to execute:\n```\ndocker exec -i ${container_id} psql -U postgres -d development -c \"INSERT INTO customers.users(col1) VALUES('record4');\"\ndocker exec -i ${container_id} psql -U postgres -d development -c \"INSERT INTO customers.users(col1) VALUES('record5');\"\n```\nRemember to replace ${container_id} with concrete container id discovered in previous step.\n12. Type in `echo $POSTGRES_PASSWORD`, we can see the password for Postgres.\n13. Type in `echo $SNOWFLAKE_PASSWORD`, we can see the password for Snowflake.\n14. Enter the data-diff command to record the result in the data_diff.csv file, which looks like this:\n```data-diff DB1_URI TABLE1_NAME DB2_URI TABLE2_NAME > diff_test.csv```\n    (1) \"DB1_URI\" is in the form of \"snowflake://[username]:[password]@[account]/[DATABASE]/[SCHEMA]?warehouse=[WAREHOUSE]&role=[ROLE]\". Remember to replace \"[xxx]\" with concrete values we recorded earlier.\n    (2) \"DB2_URI\" is in the format of \"postgresql://[username]:[password]@localhost:5432/[database]\". Remember to replace \"[xxx]\" with concrete values we recorded earlier.\n    (3) Note that, [DATABASE], [SCHEMA], [WAREHOUSE], [ROLE], and TABLE1_NAME for DB1 Snowflake should be in uppercase. The target table is \"USERS\" according to the added values;\n    (4) Note that, TABLE2_NAME for DB2 Postgres should also include the schema, that is \"customers.users\".",
      "action_number": 14
    }
  ],
  "common_patterns": {
    "opening": [
      "In this task, we need to",
      "In this task, we want to"
    ],
    "ui_navigation": [
      "Switch to",
      "Go to",
      "Click the",
      "Type in"
    ],
    "information_gathering": [
      "We will use these information later",
      "Remember these information",
      "we can see",
      "we can find"
    ],
    "multi_tool": [
      "Switch to the browser",
      "Switch to the terminal",
      "Get back to the web browser",
      "Change to"
    ],
    "terminal_commands": [
      "Type in the following codes",
      "Type in commands",
      "Enter the"
    ]
  }
}
